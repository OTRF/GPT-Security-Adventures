{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI: Basic ReAct AI Agent from Scratch\n",
    "* Collaborators:\n",
    "    * Roberto Rodriguez @Cyb3rWard0g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install openai\n",
    "! pip install python-dotenv\n",
    "! pip install pydantic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The LLM Client Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any, List\n",
    "import openai\n",
    "\n",
    "class OpenAIChatCompletion:\n",
    "    \"\"\"Interacts with OpenAI's API for chat completions.\"\"\"\n",
    "    def __init__(self, model: str, api_key: str = None, base_url: str = None):\n",
    "        self.client = openai.OpenAI(api_key=api_key, base_url=base_url)\n",
    "        self.model = model\n",
    "\n",
    "    def generate(self, messages: List[str], tools: List[Dict[str, Any]] = None, **kwargs) -> Dict[str, Any]:\n",
    "        \"\"\"Generates a response from OpenAI's API.\"\"\"\n",
    "        params = {'messages': messages, 'model': self.model, 'tools': tools, **kwargs}\n",
    "        response = self.client.chat.completions.create(**params)\n",
    "        return response.choices[0].message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Short-Term Memory Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "class ChatMessageMemory:\n",
    "    \"\"\"Manages conversation context.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.messages = []\n",
    "    \n",
    "    def add_message(self, message: Dict):\n",
    "        \"\"\"Add a message to memory.\"\"\"\n",
    "        self.messages.append(message)\n",
    "    \n",
    "    def add_messages(self, messages: List[Dict]):\n",
    "        \"\"\"Add multiple messages to memory.\"\"\"\n",
    "        for message in messages:\n",
    "            self.add_message(message)\n",
    "    \n",
    "    def add_conversation(self, user_message: Dict, assistant_message: Dict):\n",
    "        \"\"\"Add a user-assistant conversation.\"\"\"\n",
    "        self.add_messages([user_message, assistant_message])\n",
    "    \n",
    "    def get_messages(self) -> List[Dict]:\n",
    "        \"\"\"Retrieve all messages.\"\"\"\n",
    "        return self.messages.copy()\n",
    "    \n",
    "    def reset_memory(self):\n",
    "        \"\"\"Clear all messages.\"\"\"\n",
    "        self.messages = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Agent Tool Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, ValidationError\n",
    "from typing import Callable, Type\n",
    "from inspect import signature\n",
    "\n",
    "class AgentTool:\n",
    "    \"\"\"Encapsulates a Python function with Pydantic validation.\"\"\"\n",
    "    def __init__(self, func: Callable, args_model: Type[BaseModel]):\n",
    "        self.func = func\n",
    "        self.args_model = args_model\n",
    "        self.name = func.__name__\n",
    "        self.description = func.__doc__ or self.args_schema.get('description', '')\n",
    "\n",
    "    def to_openai_function_call_definition(self) -> dict:\n",
    "        \"\"\"Converts the tool to OpenAI Function Calling format.\"\"\"\n",
    "        schema_dict = self.args_schema\n",
    "        description = schema_dict.pop(\"description\", \"\")\n",
    "        return {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": self.name,\n",
    "                \"description\": description,\n",
    "                \"parameters\": schema_dict\n",
    "            }\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def args_schema(self) -> dict:\n",
    "        \"\"\"Returns the tool's function argument schema as a dictionary.\"\"\"\n",
    "        schema = self.args_model.model_json_schema()\n",
    "        schema.pop(\"title\", None)\n",
    "        return schema\n",
    "\n",
    "    def validate_json_args(self, json_string: str) -> bool:\n",
    "        \"\"\"Validate JSON string using the Pydantic model.\"\"\"\n",
    "        try:\n",
    "            validated_args = self.args_model.model_validate_json(json_string)\n",
    "            return isinstance(validated_args, self.args_model)\n",
    "        except ValidationError:\n",
    "            return False\n",
    "\n",
    "    def run(self, *args, **kwargs) -> Any:\n",
    "        \"\"\"Execute the function with validated arguments.\"\"\"\n",
    "        try:\n",
    "            # Handle positional arguments by converting them to keyword arguments\n",
    "            if args:\n",
    "                sig = signature(self.func)\n",
    "                arg_names = list(sig.parameters.keys())\n",
    "                kwargs.update(dict(zip(arg_names, args)))\n",
    "\n",
    "            # Validate arguments with the provided Pydantic schema\n",
    "            validated_args = self.args_model(**kwargs)\n",
    "            return self.func(**validated_args.model_dump())\n",
    "        except ValidationError as e:\n",
    "            raise ValueError(f\"Argument validation failed for tool '{self.name}': {str(e)}\")\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"An error occurred during the execution of tool '{self.name}': {str(e)}\")\n",
    "\n",
    "    def __call__(self, *args, **kwargs) -> Any:\n",
    "        \"\"\"Allow the AgentTool instance to be called like a regular function.\"\"\"\n",
    "        return self.run(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Tool Decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Optional, Type\n",
    "from pydantic import BaseModel\n",
    "\n",
    "def check_docstring(func: Callable):\n",
    "    \"\"\"Ensure the function has a docstring.\"\"\"\n",
    "    if not func.__doc__:\n",
    "        raise ValueError(f\"Function '{func.__name__}' must have a docstring.\")\n",
    "\n",
    "def Tool(func: Optional[Callable] = None, *, args_model: Type[BaseModel]) -> AgentTool:\n",
    "    \"\"\"Decorator to wrap a function with an AgentTool instance.\"\"\"\n",
    "    def decorator(f: Callable) -> AgentTool:\n",
    "        check_docstring(f)\n",
    "        return AgentTool(f, args_model=args_model)\n",
    "    return decorator(func) if func else decorator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Agent Tool Executor Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "class AgentToolExecutor:\n",
    "    \"\"\"Manages tool registration and execution.\"\"\"\n",
    "    \n",
    "    def __init__(self, tools: Optional[List[AgentTool]] = None):\n",
    "        self.tools: Dict[str, AgentTool] = {}\n",
    "        if tools:\n",
    "            for tool in tools:\n",
    "                self.register_tool(tool)\n",
    "    \n",
    "    def register_tool(self, tool: AgentTool):\n",
    "        \"\"\"Registers a tool.\"\"\"\n",
    "        if tool.name in self.tools:\n",
    "            raise ValueError(f\"Tool '{tool.name}' is already registered.\")\n",
    "        self.tools[tool.name] = tool\n",
    "      \n",
    "    def execute(self, tool_name: str, *args, **kwargs) -> Any:\n",
    "        \"\"\"Executes a tool by name with given arguments.\"\"\"\n",
    "        tool = self.tools.get(tool_name)\n",
    "        if not tool:\n",
    "            raise ValueError(f\"Tool '{tool_name}' not found.\")\n",
    "        try:\n",
    "            return tool(*args, **kwargs)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error executing tool '{tool_name}': {e}\") from e\n",
    "    \n",
    "    def get_tool_names(self) -> List[str]:\n",
    "        \"\"\"Returns a list of all registered tool names.\"\"\"\n",
    "        return list(self.tools.keys())\n",
    "    \n",
    "    def get_tool_details(self) -> str:\n",
    "        \"\"\"Returns details of all registered tools.\"\"\"\n",
    "        tools_info = [f\"{tool.name}: {tool.description} Args schema: {tool.args_schema['properties']}\" for tool in self.tools.values()]\n",
    "        return '\\n'.join(tools_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Previous Agent Base Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"Integrates LLM client, tools, memory, and manages tool executions.\"\"\"\n",
    "    \n",
    "    def __init__(self, llm_client, system_message: Dict[str, str], max_iterations: int = 10, tools: Optional[List[AgentTool]] = None):\n",
    "        self.llm_client = llm_client\n",
    "        self.executor = AgentToolExecutor()\n",
    "        self.memory = ChatMessageMemory()\n",
    "        self.system_message = system_message\n",
    "        self.max_iterations = max_iterations\n",
    "        self.tool_history = []\n",
    "        self.function_calls = None\n",
    "        \n",
    "        # Register and convert tools\n",
    "        if tools:\n",
    "            for tool in tools:\n",
    "                self.executor.register_tool(tool)\n",
    "            self.function_calls = [tool.to_openai_function_call_definition() for tool in tools]\n",
    "\n",
    "    def run(self, user_message: Dict[str, str]):\n",
    "        \"\"\"Generates responses, manages tool calls, and updates memory.\"\"\"\n",
    "        self.memory.add_message(user_message)\n",
    "\n",
    "        for _ in range(self.max_iterations):\n",
    "            chat_history = [self.system_message] + self.memory.get_messages() + self.tool_history\n",
    "            response = self.llm_client.generate(chat_history, tools=self.function_calls)\n",
    "\n",
    "            if self.parse_response(response):\n",
    "                continue\n",
    "            else:\n",
    "                self.memory.add_message(response)\n",
    "                self.tool_history = []\n",
    "                return response\n",
    "\n",
    "    def parse_response(self, response) -> bool:\n",
    "        \"\"\"Executes tool calls suggested by the LLM and updates tool history.\"\"\"\n",
    "        import json\n",
    "        \n",
    "        if response.tool_calls:\n",
    "            self.tool_history.append(response)\n",
    "            for tool in response.tool_calls:\n",
    "                tool_name = tool.function.name\n",
    "                tool_args = tool.function.arguments\n",
    "                tool_args_dict = json.loads(tool_args)\n",
    "                try:\n",
    "                    logger.info(f\"Executing {tool_name} with args: {tool_args}\")\n",
    "                    execution_results = self.executor.execute(tool_name, **tool_args_dict)\n",
    "                    self.tool_history.append({\n",
    "                        \"role\": \"tool\",\n",
    "                        \"tool_call_id\": tool.id,\n",
    "                        \"name\": tool_name,\n",
    "                        \"content\": str(execution_results)\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    raise ValueError(f\"Execution error in tool '{tool_name}': {e}\") from e\n",
    "            return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Tools in Prompt Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionMessage(content='{\\n    \"name\": \"greet\",\\n    \"arguments\": \"roberto\"\\n}', role='assistant', function_call=None, tool_calls=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup client with environment variables\n",
    "client = OpenAIChatCompletion(base_url='https://api.openai.com/v1', model='gpt-4')\n",
    "\n",
    "# Define the system message\n",
    "system_message = {\"role\": \"system\", \"content\": \"You are a security assistant.\"}\n",
    "\n",
    "# Initialize the Agent with the LLM client and system message\n",
    "agent = Agent(llm_client=client, system_message=system_message)\n",
    "\n",
    "USER_PROMPT = \"\"\"\n",
    "You have access to the following tools:\n",
    "\n",
    "search(topic: str) - Use it to search information.\n",
    "greet(person: str) - Use it to greet a person.\n",
    "\n",
    "Respond to the user with the right tool and input whenever is needed.\n",
    "When responding to the user, provide only ONE tool per $JSON_BLOB, as shown:\n",
    "\n",
    "```\n",
    "{{\n",
    "    \"name\": $TOOL_NAME,\n",
    "    \"arguments\": $INPUT\n",
    "}}\n",
    "```\n",
    "\n",
    "User input: Hello, this is roberto!\n",
    "\"\"\"\n",
    "\n",
    "# Define a user message\n",
    "user_message = {\"role\": \"user\", \"content\": USER_PROMPT}\n",
    "\n",
    "# Generate a response using the agent\n",
    "response = agent.run(user_message)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After receiving the response, pray 😅 and parse the message content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'greet', 'arguments': 'roberto'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "json.loads(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about JSON Within Text? 🔍\n",
    "\n",
    "Instructing an LLM to output JSON can result in mixed explanatory text and structured responses. When this happens, we can use regex patterns to extract the JSON embedded within the text.\n",
    "For parsing complex nested JSON structures, Python's re module isn't sufficient since it lacks nested pattern matching support. The regex library, however, allows recursion with patterns like  r'\\{(?:[^{}]|(?R))*\\}', enabling effective parsing of deeply nested JSON data. (https://stackoverflow.com/a/54235803)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex\n",
    "import json\n",
    "\n",
    "def parse_nested_json(text):\n",
    "    pattern = regex.compile(r'\\{(?:[^{}]|(?R))*\\}') # Supports nested structures\n",
    "    match = pattern.search(text)\n",
    "    if match:\n",
    "        try:\n",
    "            return json.loads(match.group())\n",
    "        except regex.JSONDecodeError:\n",
    "            pass\n",
    "    return None  # No valid JSON found or parsing error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a Prompt Formatter 🗣️"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The String Prompt Template Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class StringPromptTemplate:\n",
    "    \"\"\"Handles dynamic prompt formatting for an AI agent.\"\"\"\n",
    "\n",
    "    def __init__(self, template: str):\n",
    "        \"\"\"Initializes the prompt template and extracts variables.\"\"\"\n",
    "        self.template = template\n",
    "        self.variables = {}\n",
    "        self.required_variables = self.extract_variables()\n",
    "\n",
    "    def extract_variables(self):\n",
    "        \"\"\"Extracts placeholders from the template.\"\"\"\n",
    "        return set(re.findall(r'\\{(.*?)\\}', self.template))\n",
    "\n",
    "    def update_variables(self, **kwargs):\n",
    "        \"\"\"Updates template variables.\"\"\"\n",
    "        self.variables.update(kwargs)\n",
    "        self.required_variables -= set(kwargs.keys())\n",
    "\n",
    "    def format_prompt(self, **kwargs):\n",
    "        \"\"\"Generates a formatted prompt and tracks remaining variables.\"\"\"\n",
    "        combined_variables = {**self.variables, **kwargs}\n",
    "        self.required_variables -= set(kwargs.keys())\n",
    "        return self.template.format(**combined_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test the class by first updating the `USER_PROMPT` string, replacing hardcoded tools with a `{tool_details}` placeholder and initializing an instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_PROMPT = \"\"\"\n",
    "You have access to the following tools:\n",
    "\n",
    "{tool_details}\n",
    "\n",
    "Respond to the user with the right tool and input whenever is needed.\n",
    "When responding to the user, provide only ONE tool per $JSON_BLOB, as shown:\n",
    "\n",
    "```\n",
    "{{\n",
    "    \"name\": $TOOL_NAME,\n",
    "    \"arguments\": $INPUT\n",
    "}}\n",
    "```\n",
    "\n",
    "User input: What is the weather in Virginia?\n",
    "\"\"\"\n",
    "formatter = StringPromptTemplate(USER_PROMPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, define the previous tools with their respective Pydantic models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python function arguments schema\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class GetWeatherSchema(BaseModel):\n",
    "    location: str = Field(description=\"location to get weather for\")\n",
    "\n",
    "@Tool(args_model=GetWeatherSchema)\n",
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"Get weather information based on location.\"\"\"\n",
    "    return f\"{location}: 80F.\"\n",
    "\n",
    "class JumpSchema(BaseModel):\n",
    "    distance: str = Field(description=\"Distance for agent to jump\")\n",
    "\n",
    "@Tool(args_model=JumpSchema)\n",
    "def jump(distance: str) -> str:\n",
    "    \"\"\"Jump a specific distance.\"\"\"\n",
    "    return f\"I jumped the following distance {distance}\"\n",
    "\n",
    "tools = [get_weather,jump]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get tool details and replace the `tool_details` placeholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You have access to the following tools:\n",
      "\n",
      "get_weather: Get weather information based on location. Args schema: {'location': {'description': 'location to get weather for', 'title': 'Location', 'type': 'string'}}\n",
      "jump: Jump a specific distance. Args schema: {'distance': {'description': 'Distance for agent to jump', 'title': 'Distance', 'type': 'string'}}\n",
      "\n",
      "Respond to the user with the right tool and input whenever is needed.\n",
      "When responding to the user, provide only ONE tool per $JSON_BLOB, as shown:\n",
      "\n",
      "```\n",
      "{\n",
      "    \"name\": $TOOL_NAME,\n",
      "    \"arguments\": $INPUT\n",
      "}\n",
      "```\n",
      "\n",
      "User input: What is the weather in Virginia?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get tool details\n",
    "tools_info = [f\"{tool.name}: {tool.description} Args schema: {tool.args_schema['properties']}\" for tool in tools]\n",
    "tool_details = '\\n'.join(tools_info)\n",
    "\n",
    "user_prompt_formatted = formatter.format_prompt(tool_details=tool_details)\n",
    "print(user_prompt_formatted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🚦Assembly Line: Integrating Prompt Formatter with AI Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"Integrates key components and manages tool executions.\"\"\"\n",
    "    \n",
    "    def __init__(self, llm_client, system_message: Dict[str, str], max_iterations: int = 10, tools: Optional[List[AgentTool]] = None, prompt_template: StringPromptTemplate = None):\n",
    "        self.llm_client = llm_client\n",
    "        self.executor = AgentToolExecutor()\n",
    "        self.memory = ChatMessageMemory()\n",
    "        self.system_message = system_message\n",
    "        self.max_iterations = max_iterations\n",
    "        self.tool_history = []\n",
    "        self.function_calls = None\n",
    "        self.prompt_template = prompt_template\n",
    "\n",
    "        if tools:\n",
    "            for tool in tools:\n",
    "                self.executor.register_tool(tool)\n",
    "            self.function_calls = [tool.to_openai_function_call_definition() for tool in tools]\n",
    "\n",
    "        tool_details = self.executor.get_tool_details()\n",
    "        tool_names = ' or '.join(self.executor.get_tool_names())\n",
    "        self.prompt_template.update_variables(\n",
    "            system_message=self.system_message,\n",
    "            tool_details=tool_details,\n",
    "            tool_names=tool_names\n",
    "        )\n",
    "\n",
    "    def run(self, task: str):\n",
    "        \"\"\"Generates responses, manages tool calls, and updates memory.\"\"\"\n",
    "        self.memory.add_message({\"role\": \"user\", \"content\": task})\n",
    "\n",
    "        for _ in range(self.max_iterations):\n",
    "            chat_history = self.messages_to_string()\n",
    "            formatted_message = self.prompt_template.format_prompt(chat_history=chat_history, user_input=task)\n",
    "            messages = [{\"role\": \"user\", \"content\": formatted_message}]\n",
    "            response = self.llm_client.generate(messages=messages)\n",
    "            self.memory.add_conversation(user_message={\"role\": \"user\", \"content\": task}, assistant_message=response)\n",
    "            return response\n",
    "\n",
    "    def parse_response(self):\n",
    "        \"\"\"Parses LLM response.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def messages_to_string(self) -> str:\n",
    "        \"\"\"Converts messages to a string.\"\"\"\n",
    "        formatted_messages = []\n",
    "        for message in self.memory.get_messages():\n",
    "            formatted_messages.append(f\"{message['role'].capitalize()}: {message['content']}\")\n",
    "        return \"\\n\".join(formatted_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enable Tool Execution v2 ⚒️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"Integrates key components and manages tool executions.\"\"\"\n",
    "    \n",
    "    def __init__(self, llm_client, system_message: Dict[str, str], max_iterations: int = 10, tools: Optional[List[AgentTool]] = None, prompt_template: StringPromptTemplate = None):\n",
    "        self.llm_client = llm_client\n",
    "        self.executor = AgentToolExecutor()\n",
    "        self.memory = ChatMessageMemory()\n",
    "        self.system_message = system_message\n",
    "        self.max_iterations = max_iterations\n",
    "        self.tool_history = []\n",
    "        self.function_calls = None\n",
    "        self.prompt_template = prompt_template\n",
    "\n",
    "        if tools:\n",
    "            for tool in tools:\n",
    "                self.executor.register_tool(tool)\n",
    "            self.function_calls = [tool.to_openai_function_call_definition() for tool in tools]\n",
    "\n",
    "        tool_details = self.executor.get_tool_details()\n",
    "        tool_names = ' or '.join(self.executor.get_tool_names())\n",
    "        self.prompt_template.update_variables(\n",
    "            system_message=self.system_message,\n",
    "            tool_details=tool_details,\n",
    "            tool_names=tool_names\n",
    "        )\n",
    "\n",
    "    def run(self, task: str):\n",
    "        \"\"\"Generates responses, manages tool calls, and updates memory.\"\"\"\n",
    "        self.memory.add_message({\"role\": \"user\", \"content\": task})\n",
    "\n",
    "        for _ in range(self.max_iterations):\n",
    "            chat_history = self.messages_to_string()\n",
    "            formatted_message = self.prompt_template.format_prompt(chat_history=chat_history, user_input=task)\n",
    "            messages = [{\"role\": \"user\", \"content\": formatted_message}]\n",
    "            response = self.llm_client.generate(messages=messages)\n",
    "            action_dict = self.parse_response(response)\n",
    "\n",
    "            if action_dict:\n",
    "                action_name = action_dict[\"name\"]\n",
    "                action_arguments = action_dict[\"arguments\"]\n",
    "                logger.info(f\"Executing {action_name} with arguments {action_arguments}\")\n",
    "                execution_results = self.executor.execute(action_name, **action_arguments)\n",
    "                return execution_results\n",
    "            else:\n",
    "                logger.info(\"Agent is responding directly.\")\n",
    "                self.memory.add_conversation(user_message={\"role\": \"user\", \"content\": task}, assistant_message=response)\n",
    "                return response\n",
    "\n",
    "    def parse_response(self, response: Dict):\n",
    "        \"\"\"Extracts tools or continues the conversation.\"\"\"\n",
    "        import regex, json\n",
    "\n",
    "        pattern = regex.compile(r'\\{(?:[^{}]|(?R))*\\}')  # Supports nested structures\n",
    "        message_content = response.content\n",
    "        match = pattern.search(message_content)\n",
    "        if match:\n",
    "            action_content = match.group()\n",
    "            try:\n",
    "                action_dict = json.loads(action_content.strip())\n",
    "                return action_dict\n",
    "            except json.JSONDecodeError:\n",
    "                raise ValueError(\"Invalid JSON in action content\")\n",
    "        return None\n",
    "    \n",
    "    def messages_to_string(self) -> str:\n",
    "        \"\"\"Converts messages to a string.\"\"\"\n",
    "        formatted_messages = []\n",
    "        for message in self.memory.get_messages():\n",
    "            formatted_messages.append(f\"{message['role'].capitalize()}: {message['content']}\")\n",
    "        return \"\\n\".join(formatted_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can test this new agent mode with the following prompt template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "STRING_PROMPT_TEMPLATE = \"\"\"\n",
    "{system_message}. You have access to the following tools:\n",
    "\n",
    "{tool_details}\n",
    "\n",
    "Respond to the the user with the right tool and input whenever is needed.\n",
    "\n",
    "Valid tool name values: {tool_names}.\n",
    "\n",
    "When responding to the user, provide only ONE tool per $JSON_BLOB, as shown:\n",
    "\n",
    "```\n",
    "{{\n",
    "    \"name\": $TOOL_NAME,\n",
    "    \"arguments\": $INPUT\n",
    "}}\n",
    "```\n",
    "\n",
    "Previous conversation history:\n",
    "{chat_history}\n",
    "\n",
    "New conversation:\n",
    "{user_input}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the prompt template as a `StringPromptTemplate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = StringPromptTemplate(STRING_PROMPT_TEMPLATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize agent with an LLM client, system message, tools and prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM client\n",
    "client = OpenAIChatCompletion(base_url='https://api.openai.com/v1', model='gpt-4')\n",
    "\n",
    "# Define the system message\n",
    "system_message = {\"role\": \"system\", \"content\": \"You are a security assistant.\"}\n",
    "\n",
    "# Initialize the Agent with the LLM client and system message\n",
    "agent = Agent(llm_client=client, system_message=system_message, tools=tools, prompt_template=prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, run the agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'New York: 80F.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"What is the weather in New York?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReAct Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define ReAct Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# References:\n",
    "# https://smith.langchain.com/hub/hwchase17/react-chat\n",
    "# https://smith.langchain.com/hub/hwchase17/structured-chat-agent\n",
    "\n",
    "STRING_PROMPT_TEMPLATE = \"\"\"\n",
    "{system_message}. You have access to the following tools:\n",
    "\n",
    "{tool_details}\n",
    "\n",
    "Respond to the the user with the right tool and input whenever is needed.\n",
    "Use a json blob to specify a tool by providing a name key (tool name) and an arguments key (tool input).\n",
    "\n",
    "Valid \"action\" values:  {tool_names}\n",
    "\n",
    "Provide only ONE action per $JSON_BLOB, as shown:\n",
    "```\n",
    "{{\n",
    "    \"name\": $TOOL_NAME,\n",
    "    \"arguments\": $INPUT\n",
    "}}\n",
    "```\n",
    "\n",
    "Follow this format:\n",
    "\n",
    "Question: input question to answer\n",
    "Thought: reasoning about previous and subsequent steps\n",
    "Action:\n",
    "```\n",
    "$JSON_BLOB\n",
    "```\n",
    "Observation: action result\n",
    "... (repeat Thought/Action/Observation N times)\n",
    "Thought: I know what to respond as final answer. Using tool to provide final answer\n",
    "Final Answer: Final response to human\n",
    "\n",
    "Remember to ALWAYS respond with a valid json blob of a single action when a tool MUST be used.\n",
    "Use tools if necessary. Respond directly if appropriate.\n",
    "Format is Action:```$JSON_BLOB```then Observation'\n",
    "\n",
    "Previous conversation history:\n",
    "{chat_history}\n",
    "\n",
    "New conversation:\n",
    "Question: {user_input}\n",
    "{react_loop}\n",
    "\"\"\"\n",
    "prompt_template = StringPromptTemplate(STRING_PROMPT_TEMPLATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🚦Assembly Line: Integrating ReAct Prompt with AI Agent 🤖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import Dict, List, Optional\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Dict\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"\n",
    "    Basic Agent class responsible for integrating key components such as the LLM client, tools, memory, and managing tool executions.\n",
    "    \"\"\"\n",
    "    def __init__(self, llm_client, system_message: Dict[str, str], max_iterations: int = 10, tools: Optional[List[AgentTool]] = None, prompt_template : StringPromptTemplate=None):\n",
    "        self.llm_client = llm_client\n",
    "        self.executor = AgentToolExecutor()\n",
    "        self.memory = ChatMessageMemory()\n",
    "        self.system_message = system_message\n",
    "        self.max_iterations = max_iterations\n",
    "        self.tool_history = []\n",
    "        self.function_calls = None\n",
    "        self.prompt_template = prompt_template  # Instance of StringPromptTemplate or similar\n",
    "        \n",
    "        # Register each tool passed to the Agent using the executor\n",
    "        if tools:\n",
    "            for tool in tools:\n",
    "                # Register Agent Tools\n",
    "                self.executor.register_tool(tool)\n",
    "                # Convert Agent Tools\n",
    "                self.function_calls = [tool.to_openai_function_call_definition() for tool in tools]\n",
    "        \n",
    "        # Pre-fill the prompt template with initial variables\n",
    "        tool_details=self.executor.get_tool_details(),\n",
    "        tool_names=' or '.join(self.executor.get_tool_names())\n",
    "        self.prompt_template.update_variables(\n",
    "            system_message=self.system_message,\n",
    "            tool_details=tool_details,\n",
    "            tool_names=tool_names\n",
    "        )\n",
    "\n",
    "    def run(self, task:str):\n",
    "        # Get Chat History\n",
    "        chat_history = self.messages_to_string()\n",
    "        # Initialize ReAct Loop\n",
    "        react_loop = \"\"\n",
    "\n",
    "        # Showing Initial Task\n",
    "        print(f\"Question: {task}\")\n",
    "\n",
    "        for _ in range(self.max_iterations):\n",
    "            # Generate a dynamic prompt using variables\n",
    "            formatted_message = self.prompt_template.format_prompt(chat_history=chat_history,user_input=task,react_loop=react_loop)\n",
    "            # Define everything as a user message\n",
    "            messages = [{\"role\":\"user\", \"content\": formatted_message}]\n",
    "            \n",
    "            # Instruct LLM to choose the right tool and respond with a structured output\n",
    "            response = self.llm_client.generate(messages=messages,stop=[\"\\nObservation:\"],)\n",
    "            # Parse response and extract tools if any\n",
    "            action_dict = self.parse_response(response)\n",
    "\n",
    "            if action_dict:\n",
    "                action_name = action_dict[\"name\"]\n",
    "                action_arguments = action_dict[\"arguments\"]\n",
    "                \n",
    "                logger.info(f\"Executing {action_name} with arguments {action_arguments}\")\n",
    "                execution_results = self.executor.execute(action_name, **action_arguments)\n",
    "                \n",
    "                current_iteration = f\"\\nThought: {response.content}\\nObservation: {execution_results}\"\n",
    "                react_loop += current_iteration\n",
    "                print(current_iteration)\n",
    "            else:\n",
    "                message_content = response.content\n",
    "                print(message_content)\n",
    "                if 'final answer' in str(message_content).lower():\n",
    "                    final_message = str(message_content).lower().split(\"final answer:\")[-1].strip()\n",
    "                    response = {\n",
    "                        \"role\": \"assistant\",\n",
    "                        \"content\": final_message\n",
    "                    }\n",
    "                \n",
    "                logger.info(\"Agent is responding directly.\")\n",
    "                self.memory.add_conversation(\n",
    "                    user_message={\"role\":\"user\",\"content\": task},\n",
    "                    assistant_message=response\n",
    "                )\n",
    "                return response    \n",
    "\n",
    "    def parse_response(self, response: Dict):\n",
    "        \"\"\"\n",
    "        Extracts tools or continues the conversation.\n",
    "        \"\"\"\n",
    "        import regex, json\n",
    "\n",
    "        pattern = regex.compile(r'\\{(?:[^{}]|(?R))*\\}') # Supports nested structures\n",
    "        message_content = response.content\n",
    "        match = pattern.search(message_content)\n",
    "        if match:\n",
    "            action_content = match.group()\n",
    "            try:\n",
    "                action_dict = json.loads(action_content.strip())\n",
    "                return action_dict\n",
    "            except regex.JSONDecodeError:\n",
    "                raise ValueError(\"Invalid JSON in action content\")\n",
    "    \n",
    "    def messages_to_string(self) -> str:\n",
    "        \"\"\"\n",
    "        Converts a list of message objects or dictionaries into a multi-line string representation.\n",
    "        \"\"\"\n",
    "        formatted_messages = []\n",
    "\n",
    "        for message in self.memory.get_messages():\n",
    "            if isinstance(message, BaseModel):\n",
    "                message = message.model_dump()\n",
    "            formatted_messages.append(f\"{message[\"role\"].capitalize()}: {message[\"content\"]}\")\n",
    "        return \"\\n\".join(formatted_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any, List\n",
    "import openai\n",
    "\n",
    "class OpenAIChatCompletion:\n",
    "    \"\"\"Interacts with OpenAI's API for chat completions.\"\"\"\n",
    "    def __init__(self, model: str, api_key: str = None, base_url: str = None):\n",
    "        self.client = openai.OpenAI(api_key=api_key, base_url=base_url)\n",
    "        self.model = model\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        messages: List[str],\n",
    "        tools: List[Dict[str, Any]] = None,\n",
    "        stop: List[str] = None,\n",
    "        **kwargs) -> Dict[str, Any]:\n",
    "        \"\"\"Generates a response from OpenAI's API.\"\"\"\n",
    "        params = {\n",
    "            'messages': messages,\n",
    "            'model': self.model,\n",
    "            'tools': tools,\n",
    "            'stop': stop,\n",
    "            **kwargs\n",
    "        }\n",
    "        response = self.client.chat.completions.create(**params)\n",
    "        return response.choices[0].message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I updated the LLM Client class to use the stop parameter in [OpenAI's Chat Completion API](https://platform.openai.com/docs/api-reference/chat/create), stopping token generation at `\\nObservation`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM client\n",
    "client = OpenAIChatCompletion(base_url='https://api.openai.com/v1', model='gpt-4')\n",
    "\n",
    "# Define the system message\n",
    "system_message = {\"role\": \"system\", \"content\": \"You are a security assistant.\"}\n",
    "\n",
    "# Initialize the Agent with the LLM client and system message\n",
    "agent = Agent(llm_client=client, system_message=system_message, tools=tools, prompt_template=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the weather in New York?\n",
      "\n",
      "Thought: Thought: The user is asking for the weather information in New York. I will use the \"get_weather\" tool to provide this information.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "    \"name\": \"get_weather\",\n",
      "    \"arguments\": {\"location\": \"New York\"}\n",
      "}\n",
      "```\n",
      "Observation: New York: 80F.\n",
      "Thought: I observed that the tool successfully retrieved the current temperature in New York, to which is 80F.  Now I can respond to the user's query.\n",
      "\n",
      "Final Answer: The current weather in New York is 80F.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant', 'content': 'the current weather in new york is 80f.'}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"What is the weather in New York?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'What is the weather in New York?'},\n",
       " {'role': 'assistant', 'content': 'the current weather in new york is 80f.'}]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.memory.get_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.memory.reset_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the weather in New York, Paris, and Virginia?\n",
      "\n",
      "Thought: Thought: The user wants to know the weather in three different locations: New York, Paris, and Virginia. I have a tool for fetching the weather information, but it requires a string for a single location as an argument. I will need to handle each location one by one before providing a final response.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "    \"name\": \"get_weather\",\n",
      "    \"arguments\": {\"location\": \"New York\"}\n",
      "}\n",
      "```\n",
      "Observation: New York: 80F.\n",
      "\n",
      "Thought: Thought: I have gathered the weather information for New York. Now I need to get the weather for Paris.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "    \"name\": \"get_weather\",\n",
      "    \"arguments\": {\"location\": \"Paris\"}\n",
      "}\n",
      "```\n",
      "Observation: Paris: 80F.\n",
      "\n",
      "Thought: Thought: I have gathered the weather information for New York and Paris. Now I need to get the weather for Virginia.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "    \"name\": \"get_weather\",\n",
      "    \"arguments\": {\"location\": \"Virginia\"}\n",
      "}\n",
      "```\n",
      "Observation: Virginia: 80F.\n",
      "Thought: Now I have the weather information for all the three locations that the user asked for: New York, Paris, and Virginia. I can now provide the final response.\n",
      "\n",
      "Final Answer: The current weather is 80F in New York, 80F in Paris, and 80F in Virginia.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant',\n",
       " 'content': 'the current weather is 80f in new york, 80f in paris, and 80f in virginia.'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"What is the weather in New York, Paris, and Virginia?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
