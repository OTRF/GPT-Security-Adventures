{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Basic Tool Calling AI Agent from Scratch\n",
    "* Collaborators:\n",
    "    * Roberto Rodriguez @Cyb3rWard0g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install openai\n",
    "! pip install python-dotenv\n",
    "! pip install pydantic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Agent Base Class ðŸ¤–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\"\n",
    "    Integrates LLM client, tools, and memory.\n",
    "    \"\"\"\n",
    "    def __init__(self, llm_client=None, tools=None, memory=None):\n",
    "        self.llm_client = llm_client\n",
    "        self.tools = tools\n",
    "        self.memory = memory\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Placeholder for the agent's main execution logic.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the LLM Client ðŸ“¡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The OpenAI Chat Completion Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any, List\n",
    "import openai\n",
    "\n",
    "class OpenAIChatCompletion:\n",
    "    \"\"\"\n",
    "    Interacts with OpenAI's API for chat completions.\n",
    "    \"\"\"\n",
    "    def __init__(self, model: str, api_key: str = None, base_url: str = None):\n",
    "        \"\"\"\n",
    "        Initialize with model, API key, and base URL.\n",
    "        \"\"\"\n",
    "        self.client = openai.OpenAI(api_key=api_key, base_url=base_url)\n",
    "        self.model = model\n",
    "\n",
    "    def generate(self, messages: List[Dict], **kwargs) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Generate a response from input messages.\n",
    "        \"\"\"\n",
    "        params = {'messages': messages, 'model': self.model, **kwargs}\n",
    "        response = self.client.chat.completions.create(**params)\n",
    "        return response.choices[0].message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ðŸš¦Assembly Line: Integrating LLM Client with AI Agent ðŸ¤–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\"\n",
    "    Integrates LLM client, tools, and memory.\n",
    "    \"\"\"\n",
    "    def __init__(self, llm_client, tools=None, memory=None):\n",
    "        self.llm_client = llm_client\n",
    "        self.tools = tools\n",
    "        self.memory = memory\n",
    "\n",
    "    def run(self, messages: List[Dict[str, str]]):\n",
    "        \"\"\"\n",
    "        Generates a response from the LLM client.\n",
    "        \"\"\"\n",
    "        # Generate response using the LLM client\n",
    "        response = self.llm_client.generate(messages)\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Updated Agent Class ðŸ”¬\n",
    "\n",
    "To securely handle the API key, set the `OPENAI_API_KEY` variable in a .env file and use [Python-dotenv](https://pypi.org/project/python-dotenv/) to load it as an environment variable. This method avoids hardcoding sensitive information in your source code. The OpenAI library will automatically detect and use this variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # Load environment variables from .env."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, initialize the new class, define a few messages, and run the agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='Hello Roberto! How can I assist you today?', role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the OpenAI client\n",
    "client = OpenAIChatCompletion(base_url='https://api.openai.com/v1', model='gpt-4')\n",
    "\n",
    "# Define a set of messages\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a security assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hey! This is Roberto!\"}\n",
    "]\n",
    "\n",
    "# Initialize the agent with the LLM client\n",
    "myAgent = Agent(llm_client=client)\n",
    "\n",
    "# Use the agent to run a task\n",
    "response = myAgent.run(messages=messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Short-Term Memory ðŸ’¾"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Short-Term Memory Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "class ChatMessageMemory:\n",
    "    \"\"\"Manages conversation context.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.messages = []\n",
    "    \n",
    "    def add_message(self, message: Dict):\n",
    "        \"\"\"Add a message to memory.\"\"\"\n",
    "        self.messages.append(message)\n",
    "    \n",
    "    def add_messages(self, messages: List[Dict]):\n",
    "        \"\"\"Add multiple messages to memory.\"\"\"\n",
    "        for message in messages:\n",
    "            self.add_message(message)\n",
    "    \n",
    "    def add_conversation(self, user_message: Dict, assistant_message: Dict):\n",
    "        \"\"\"Add a user-assistant conversation.\"\"\"\n",
    "        self.add_messages([user_message, assistant_message])\n",
    "    \n",
    "    def get_messages(self) -> List[Dict]:\n",
    "        \"\"\"Retrieve all messages.\"\"\"\n",
    "        return self.messages.copy()\n",
    "    \n",
    "    def reset_memory(self):\n",
    "        \"\"\"Clear all messages.\"\"\"\n",
    "        self.messages = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸš¦Assembly Line: Integrating Memory with AI Agent ðŸ¤–\n",
    "\n",
    "With the LLM client set up and the memory class ready, the next step was to add memory into the Agent class. Now, the agent initializes with the system message, and user messages are added to memory before merging with the system message. This allows efficient generation, storage, and retrieval of conversation context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\"Integrates LLM client, tools, and memory.\"\"\"\n",
    "    \n",
    "    def __init__(self, llm_client, system_message: Dict[str, str], tools=None):\n",
    "        self.llm_client = llm_client\n",
    "        self.tools = tools\n",
    "        self.memory = ChatMessageMemory()\n",
    "        self.system_message = system_message\n",
    "\n",
    "    def run(self, user_message: Dict[str, str]):\n",
    "        \"\"\"Generate a response using LLM client and store context.\"\"\"\n",
    "        self.memory.add_message(user_message)\n",
    "        chat_history = [self.system_message] + self.memory.get_messages()\n",
    "        response = self.llm_client.generate(chat_history)\n",
    "        self.memory.add_message(response)\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Updated Agent Class ðŸ”¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='Your name is Roberto.', role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "# Test the updated agent\n",
    "client = OpenAIChatCompletion(base_url='https://api.openai.com/v1', model='gpt-4')\n",
    "\n",
    "# Define the system message\n",
    "system_message = {\"role\": \"system\", \"content\": \"You are a security assistant.\"}\n",
    "\n",
    "# Initialize the Agent with the LLM client and system message\n",
    "agent = Agent(llm_client=client, system_message=system_message)\n",
    "\n",
    "# Define a user message\n",
    "user_message = {\"role\": \"user\", \"content\": \"Hey! This is Roberto!\"}\n",
    "\n",
    "# Generate a response using the agent\n",
    "response = agent.run(user_message)\n",
    "\n",
    "# Add a follow-up message to test memory integration\n",
    "follow_up_message = {\"role\": \"user\", \"content\": \"What was my name?\"}\n",
    "\n",
    "# Run the agent again with the new message\n",
    "response = agent.run(follow_up_message)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enabling Tool Execution ðŸ› ï¸"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing OpenAI's Function Calling Capability ðŸ”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1ï¸âƒ£ Update Open AI Chat Completion Client\n",
    "\n",
    "First, I needed to update the client to accept an additional tools parameter, allowing users to pass the tool details along with the user message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any, List\n",
    "import openai\n",
    "\n",
    "class OpenAIChatCompletion:\n",
    "    \"\"\"Interacts with OpenAI's API for chat completions.\"\"\"\n",
    "    def __init__(self, model: str, api_key: str = None, base_url: str = None):\n",
    "        self.client = openai.OpenAI(api_key=api_key, base_url=base_url)\n",
    "        self.model = model\n",
    "\n",
    "    def generate(self, messages: List[str], tools: List[Dict[str, Any]] = None, **kwargs) -> Dict[str, Any]:\n",
    "        \"\"\"Generates a response from OpenAI's API.\"\"\"\n",
    "        params = {'messages': messages, 'model': self.model, 'tools': tools, **kwargs}\n",
    "        response = self.client.chat.completions.create(**params)\n",
    "        return response.choices[0].message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2ï¸âƒ£ Update Agent Class to Integrate Tools\n",
    "\n",
    "Next, I updated the Agent class to initialize it with tools and pass those tools to the LLM client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"Integrates LLM client, tools, and memory.\"\"\"\n",
    "    def __init__(self, llm_client, system_message: Dict[str, str], tools=None):\n",
    "        self.llm_client = llm_client\n",
    "        self.tools = tools\n",
    "        self.memory = ChatMessageMemory()\n",
    "        self.system_message = system_message\n",
    "\n",
    "    def run(self, user_message: Dict[str, str]):\n",
    "        self.memory.add_message(user_message)\n",
    "        chat_history = [self.system_message] + self.memory.get_messages()\n",
    "        response = self.llm_client.generate(chat_history, tools=self.tools)\n",
    "        self.memory.add_message(response)\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3ï¸âƒ£ Define Dummy Python Functions\n",
    "\n",
    "I defined a few dummy Python functions to test these capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"Gets weather information.\"\"\"\n",
    "    return f\"{location}: 80F.\"\n",
    "\n",
    "def jump(distance: str) -> str:\n",
    "    \"\"\"Jumps a specific distance.\"\"\"\n",
    "    return f\"I jumped the following distance {distance}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4ï¸âƒ£ Define the Function Signature as OpenAI Function Calls\n",
    "\n",
    "I then defined a dictionary for each tool following the expected schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_weather_func_dict = {\n",
    "    'type': 'function',\n",
    "    'function': {\n",
    "        'name': 'get_weather',\n",
    "        'description': 'Get weather information based on location.',\n",
    "        'parameters': {\n",
    "            'properties': {'location': {'type': 'string'}},\n",
    "            'required': ['location'],\n",
    "            'type': 'object'\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "jump_func_dict = {\n",
    "    'type': 'function',\n",
    "    'function': {\n",
    "        'name': 'jump',\n",
    "        'description': 'Jump a specific distance.',\n",
    "        'parameters': {\n",
    "            'properties': {'distance': {'type': 'string'}},\n",
    "            'required': ['distance'],\n",
    "            'type': 'object'\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5ï¸âƒ£ Initialize Agent and Test It\n",
    "\n",
    "I then initialized the client and used it to generate a response from the LLM, including tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_HFyUnaAmRc9trG4HdBwdjg7v', function=Function(arguments='{\\n  \"location\": \"Virginia\"\\n}', name='get_weather'), type='function')])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = OpenAIChatCompletion(base_url='https://api.openai.com/v1', model='gpt-4')\n",
    "\n",
    "# Define the system message\n",
    "system_message = {\"role\": \"system\", \"content\": \"You are a security assistant.\"}\n",
    "\n",
    "# Gather all available tools\n",
    "tools = [get_weather_func_dict, jump_func_dict]\n",
    "\n",
    "# Initialize the Agent with the LLM client, system message, and tools\n",
    "agent = Agent(llm_client=client, system_message=system_message, tools=tools)\n",
    "\n",
    "# Define a user message\n",
    "user_message = {\"role\": \"user\", \"content\": \"What is the weather in Virginia?\"}\n",
    "\n",
    "# Generate a response using the agent\n",
    "response = agent.run(user_message)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6ï¸âƒ£ Capture Assistant's Response and Extract Tool Details\n",
    "\n",
    "The LLM returned a message with the tool details and suggested arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'location': 'Virginia'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "tool_response = response.tool_calls[0]\n",
    "tool_arguments = json.loads(tool_response.function.arguments)\n",
    "tool_arguments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7ï¸âƒ£ Execute the Tool with the Function Arguments\n",
    "\n",
    "I then executed the tool with the suggested arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Virginia: 80F.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_execution_results = get_weather(**tool_arguments)\n",
    "tool_execution_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8ï¸âƒ£ Craft a Tool Message and Send It to the LLM\n",
    "\n",
    "Finally, I crafted a Tool message with the results and sent it to the LLM for a final response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionMessage(content='The current weather in Virginia is 80Â°F.', role='assistant', function_call=None, tool_calls=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_message = {\n",
    "    \"role\": \"tool\",\n",
    "    \"tool_call_id\": tool_response.id,\n",
    "    \"name\": tool_response.function.name,\n",
    "    \"content\": str(tool_execution_results)\n",
    "}\n",
    "\n",
    "final_response = agent.run(tool_message)\n",
    "final_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enter Pydantic: Validating Tool Arguments ðŸ”\n",
    "\n",
    "[Pydantic](https://docs.pydantic.dev/latest/) is a powerful library that leverages Python type hints for efficient data validation and serialization. Using [Pydantic models](https://docs.pydantic.dev/latest/concepts/models/), we can define and validate tool arguments, ensuring correct data types and structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Tool Argument Schema as a Pydantic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python function\n",
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"Get weather information based on location.\"\"\"\n",
    "    return f\"{location}: 80F.\"\n",
    "\n",
    "# Pydantic Model\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class GetWeatherSchema(BaseModel):\n",
    "    \"\"\"Get weather information based on location.\"\"\"\n",
    "    location: str = Field(description=\"Location to get weather for\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate Suggested Tool Argument Schema\n",
    "\n",
    "To validate the suggested tool argument schema, follow the previous steps up to initializing the agent and generating a response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6ï¸âƒ£ Capture Assistant's Response and Extract Tool Details\n",
    "\n",
    "The LLM returned a message with the tool details and suggested arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract tool arguments (JSON string)\n",
    "tool_response = response.tool_calls[0]\n",
    "tool_arguments = tool_response.function.arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of manually converting the JSON string, validate it directly with Pydantic's [model_validate_json()](https://docs.pydantic.dev/latest/api/base_model/#pydantic.BaseModel.model_validate_json) utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GetWeatherSchema(location='Virginia')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_model = GetWeatherSchema.model_validate_json(tool_arguments)\n",
    "response_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also generate JSON schemas from Pydantic models using Pydantic's [model_json_schema()](https://docs.pydantic.dev/latest/api/base_model/#pydantic.BaseModel.model_json_schema) utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'description': 'Get weather information based on location.',\n",
       " 'properties': {'location': {'description': 'Location to get weather for',\n",
       "   'title': 'Location',\n",
       "   'type': 'string'}},\n",
       " 'required': ['location'],\n",
       " 'title': 'GetWeatherSchema',\n",
       " 'type': 'object'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GetWeatherSchema.model_json_schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatting Tools for OpenAI Function Calls with Pydantic Models ðŸ”¥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Pydantic Models to OpenAI Functions \n",
    "\n",
    "I used the previous Pydantic model GetWeatherSchema and created a function to convert the tool's argument schema into OpenAI's Function Calling format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_openai_function_call_definition(name: str, model: BaseModel):\n",
    "    schema_dict = model.model_json_schema()\n",
    "    description = schema_dict.pop(\"description\", \"\")\n",
    "    schema_dict.pop(\"title\", None)  # Remove the title field to exclude the model name\n",
    "    return {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": name,\n",
    "            \"description\": description,\n",
    "            \"parameters\": schema_dict\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing this new capability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'function',\n",
       " 'function': {'name': 'get_weather',\n",
       "  'description': 'Get weather information based on location.',\n",
       "  'parameters': {'properties': {'location': {'description': 'Location to get weather for',\n",
       "     'title': 'Location',\n",
       "     'type': 'string'}},\n",
       "   'required': ['location'],\n",
       "   'type': 'object'}}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function_call_definition = to_openai_function_call_definition(\"get_weather\", GetWeatherSchema)\n",
    "function_call_definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streamlining the Agent Tool Concept âš’ï¸\n",
    "\n",
    "To streamline the process of converting Python functions to OpenAI's Function Calling format, I created a dedicated class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Agent Tool Class\n",
    "\n",
    "I developed the AgentTool class to encapsulate a Python function and its Pydantic model. This class includes methods for converting the tool to OpenAI's format and generating a JSON schema of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import Callable, Type\n",
    "\n",
    "class AgentTool:\n",
    "    \"\"\"Encapsulates a Python function with Pydantic validation.\"\"\"\n",
    "    def __init__(self, func: Callable, args_model: Type[BaseModel]):\n",
    "        self.func = func\n",
    "        self.args_model = args_model\n",
    "        self.name = func.__name__\n",
    "        self.description = func.__doc__ or self.args_schema.get('description', '')\n",
    "\n",
    "    def to_openai_function_call_definition(self) -> dict:\n",
    "        \"\"\"Converts the tool to OpenAI Function Calling format.\"\"\"\n",
    "        schema_dict = self.args_schema\n",
    "        description = schema_dict.pop(\"description\", \"\")\n",
    "        return {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": self.name,\n",
    "                \"description\": description,\n",
    "                \"parameters\": schema_dict\n",
    "            }\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def args_schema(self) -> dict:\n",
    "        \"\"\"Returns the tool's function argument schema as a dictionary.\"\"\"\n",
    "        schema = self.args_model.model_json_schema()\n",
    "        schema.pop(\"title\", None)\n",
    "        return schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Tool Decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Optional, Type\n",
    "from pydantic import BaseModel\n",
    "\n",
    "def check_docstring(func: Callable):\n",
    "    \"\"\"Ensure the function has a docstring.\"\"\"\n",
    "    if not func.__doc__:\n",
    "        raise ValueError(f\"Function '{func.__name__}' must have a docstring.\")\n",
    "\n",
    "def Tool(func: Optional[Callable] = None, *, args_model: Type[BaseModel]) -> AgentTool:\n",
    "    \"\"\"Decorator to wrap a function with an AgentTool instance.\"\"\"\n",
    "    def decorator(f: Callable) -> AgentTool:\n",
    "        check_docstring(f)\n",
    "        return AgentTool(f, args_model=args_model)\n",
    "    return decorator(func) if func else decorator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could then use the Tool decorator to create an AgentTool and convert it to OpenAI's Function Calling format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'function',\n",
       " 'function': {'name': 'get_weather',\n",
       "  'description': '',\n",
       "  'parameters': {'properties': {'location': {'description': 'location to get weather for',\n",
       "     'title': 'Location',\n",
       "     'type': 'string'}},\n",
       "   'required': ['location'],\n",
       "   'type': 'object'}}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Python function arguments schema\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class GetWeatherSchema(BaseModel):\n",
    "    location: str = Field(description=\"location to get weather for\")\n",
    "\n",
    "@Tool(args_model=GetWeatherSchema)\n",
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"Get weather information based on location.\"\"\"\n",
    "    return f\"{location}: 80F.\"\n",
    "\n",
    "# Convert the AgentTool to OpenAI's Function Calling format\n",
    "get_weather.to_openai_function_call_definition()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validating Tool Arguments before Agent Tool Execution\n",
    "\n",
    "To ensure tool arguments were validated before execution, I added functionality to the AgentTool class. This involved creating a method to validate JSON strings using Pydantic Pydantic's model_validate_json() utility and modifying the call function to validate parameters before executing the tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, ValidationError\n",
    "from typing import Callable, Type\n",
    "from inspect import signature\n",
    "\n",
    "class AgentTool:\n",
    "    \"\"\"Encapsulates a Python function with Pydantic validation.\"\"\"\n",
    "    def __init__(self, func: Callable, args_model: Type[BaseModel]):\n",
    "        self.func = func\n",
    "        self.args_model = args_model\n",
    "        self.name = func.__name__\n",
    "        self.description = func.__doc__ or self.args_schema.get('description', '')\n",
    "\n",
    "    def to_openai_function_call_definition(self) -> dict:\n",
    "        \"\"\"Converts the tool to OpenAI Function Calling format.\"\"\"\n",
    "        schema_dict = self.args_schema\n",
    "        description = schema_dict.pop(\"description\", \"\")\n",
    "        return {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": self.name,\n",
    "                \"description\": description,\n",
    "                \"parameters\": schema_dict\n",
    "            }\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def args_schema(self) -> dict:\n",
    "        \"\"\"Returns the tool's function argument schema as a dictionary.\"\"\"\n",
    "        schema = self.args_model.model_json_schema()\n",
    "        schema.pop(\"title\", None)\n",
    "        return schema\n",
    "\n",
    "    def validate_json_args(self, json_string: str) -> bool:\n",
    "        \"\"\"Validate JSON string using the Pydantic model.\"\"\"\n",
    "        try:\n",
    "            validated_args = self.args_model.model_validate_json(json_string)\n",
    "            return isinstance(validated_args, self.args_model)\n",
    "        except ValidationError:\n",
    "            return False\n",
    "\n",
    "    def run(self, *args, **kwargs) -> Any:\n",
    "        \"\"\"Execute the function with validated arguments.\"\"\"\n",
    "        try:\n",
    "            # Handle positional arguments by converting them to keyword arguments\n",
    "            if args:\n",
    "                sig = signature(self.func)\n",
    "                arg_names = list(sig.parameters.keys())\n",
    "                kwargs.update(dict(zip(arg_names, args)))\n",
    "\n",
    "            # Validate arguments with the provided Pydantic schema\n",
    "            validated_args = self.args_model(**kwargs)\n",
    "            return self.func(**validated_args.model_dump())\n",
    "        except ValidationError as e:\n",
    "            raise ValueError(f\"Argument validation failed for tool '{self.name}': {str(e)}\")\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"An error occurred during the execution of tool '{self.name}': {str(e)}\")\n",
    "\n",
    "    def __call__(self, *args, **kwargs) -> Any:\n",
    "        \"\"\"Allow the AgentTool instance to be called like a regular function.\"\"\"\n",
    "        return self.run(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the new methods in the AgentTool class, you can now validate tool arguments and execute the function with validated parameters. Similar to the previous example, initialize an AgentTool with its schema model using the Tool decorator. Then, validate a dummy tool argument JSON string and run the tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python function arguments schema\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class GetWeatherSchema(BaseModel):\n",
    "    location: str = Field(description=\"location to get weather for\")\n",
    "\n",
    "@Tool(args_model=GetWeatherSchema)\n",
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"Get weather information based on location.\"\"\"\n",
    "    return f\"{location}: 80F.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Virginia: 80F.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_json_arguments = '{\\n  \"location\": \"Virginia\"\\n}'\n",
    "\n",
    "# Validate the JSON string arguments\n",
    "get_weather.validate_json_args(tool_json_arguments)  # True\n",
    "\n",
    "# Convert JSON string to dictionary and run the tool\n",
    "tool_arguments_dict = json.loads(tool_json_arguments)\n",
    "get_weather.run(**tool_arguments_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operationalizing Tool Execution âš’ï¸ -> âš¡ï¸\n",
    "\n",
    "After setting up our agent to select and prepare tools with the help of LLMs, the next step was to manage these tools effectively. It was essential to have a system that could handle tool availability and execution seamlessly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Agent Tool Executor Class\n",
    "\n",
    "I created an AgentToolExecutor class to manage the agent's interaction with tools. This class ensures tools are registered, managed, and executed properly, bridging the gap between choosing the right tool and executing it effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "class AgentToolExecutor:\n",
    "    \"\"\"Manages tool registration and execution.\"\"\"\n",
    "    \n",
    "    def __init__(self, tools: Optional[List[AgentTool]] = None):\n",
    "        self.tools: Dict[str, AgentTool] = {}\n",
    "        if tools:\n",
    "            for tool in tools:\n",
    "                self.register_tool(tool)\n",
    "    \n",
    "    def register_tool(self, tool: AgentTool):\n",
    "        \"\"\"Registers a tool.\"\"\"\n",
    "        if tool.name in self.tools:\n",
    "            raise ValueError(f\"Tool '{tool.name}' is already registered.\")\n",
    "        self.tools[tool.name] = tool\n",
    "      \n",
    "    def execute(self, tool_name: str, *args, **kwargs) -> Any:\n",
    "        \"\"\"Executes a tool by name with given arguments.\"\"\"\n",
    "        tool = self.tools.get(tool_name)\n",
    "        if not tool:\n",
    "            raise ValueError(f\"Tool '{tool_name}' not found.\")\n",
    "        try:\n",
    "            return tool(*args, **kwargs)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error executing tool '{tool_name}': {e}\") from e\n",
    "    \n",
    "    def get_tool_names(self) -> List[str]:\n",
    "        \"\"\"Returns a list of all registered tool names.\"\"\"\n",
    "        return list(self.tools.keys())\n",
    "    \n",
    "    def get_tool_details(self) -> str:\n",
    "        \"\"\"Returns details of all registered tools.\"\"\"\n",
    "        tools_info = [f\"{tool.name}: {tool.description} Args schema: {tool.args_schema['properties']}\" for tool in self.tools.values()]\n",
    "        return '\\n'.join(tools_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸš¦Assembly Line: Integrating Tool Executor with AI Agent ðŸ¤–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"Integrates LLM client, tools, memory, and manages tool executions.\"\"\"\n",
    "    \n",
    "    def __init__(self, llm_client, system_message: Dict[str, str], max_iterations: int = 10, tools: Optional[List[AgentTool]] = None):\n",
    "        self.llm_client = llm_client\n",
    "        self.executor = AgentToolExecutor()\n",
    "        self.memory = ChatMessageMemory()\n",
    "        self.system_message = system_message\n",
    "        self.max_iterations = max_iterations\n",
    "        self.tool_history = []\n",
    "        self.function_calls = None\n",
    "        \n",
    "        # Register and convert tools\n",
    "        if tools:\n",
    "            for tool in tools:\n",
    "                self.executor.register_tool(tool)\n",
    "            self.function_calls = [tool.to_openai_function_call_definition() for tool in tools]\n",
    "\n",
    "    def run(self, user_message: Dict[str, str]):\n",
    "        \"\"\"Generates responses, manages tool calls, and updates memory.\"\"\"\n",
    "        self.memory.add_message(user_message)\n",
    "\n",
    "        for _ in range(self.max_iterations):\n",
    "            chat_history = [self.system_message] + self.memory.get_messages() + self.tool_history\n",
    "            response = self.llm_client.generate(chat_history, tools=self.function_calls)\n",
    "\n",
    "            if self.parse_response(response):\n",
    "                continue\n",
    "            else:\n",
    "                self.memory.add_message(response)\n",
    "                self.tool_history = []\n",
    "                return response\n",
    "\n",
    "    def parse_response(self, response) -> bool:\n",
    "        \"\"\"Executes tool calls suggested by the LLM and updates tool history.\"\"\"\n",
    "        import json\n",
    "        \n",
    "        if response.tool_calls:\n",
    "            self.tool_history.append(response)\n",
    "            for tool in response.tool_calls:\n",
    "                tool_name = tool.function.name\n",
    "                tool_args = tool.function.arguments\n",
    "                tool_args_dict = json.loads(tool_args)\n",
    "                try:\n",
    "                    logger.info(f\"Executing {tool_name} with args: {tool_args}\")\n",
    "                    execution_results = self.executor.execute(tool_name, **tool_args_dict)\n",
    "                    self.tool_history.append({\n",
    "                        \"role\": \"tool\",\n",
    "                        \"tool_call_id\": tool.id,\n",
    "                        \"name\": tool_name,\n",
    "                        \"content\": str(execution_results)\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    raise ValueError(f\"Execution error in tool '{tool_name}': {e}\") from e\n",
    "            return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the new Agent base class, follow the steps in previous examples. Initialize the client, define your tools with the Tool decorator, initialize the agent with those tools and the LLM client, and finally, run the agent with your query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Executing get_weather with args: {\n",
      "  \"location\": \"Virginia\"\n",
      "}\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatCompletionMessage(content='The weather in Virginia is currently 80Â°F.', role='assistant', function_call=None, tool_calls=None)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set Environment Variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # take environment variables from .env.\n",
    "\n",
    "# Initialize LLM Client\n",
    "client = OpenAIChatCompletion(base_url='https://api.openai.com/v1', model='gpt-4')\n",
    "\n",
    "# Initialize Agent with tools and LLM client\n",
    "\n",
    "# Python function arguments schema\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class GetWeatherSchema(BaseModel):\n",
    "    location: str = Field(description=\"location to get weather for\")\n",
    "\n",
    "@Tool(args_model=GetWeatherSchema)\n",
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"Get weather information based on location.\"\"\"\n",
    "    return f\"{location}: 80F.\"\n",
    "\n",
    "class JumpSchema(BaseModel):\n",
    "    distance: str = Field(description=\"distance to jump.\")\n",
    "\n",
    "@Tool(args_model=JumpSchema)\n",
    "def jump(distance: str) -> str:\n",
    "    \"\"\"Jumps a specific distance.\"\"\"\n",
    "    return f\"I jumped the following distance {distance}\"\n",
    "\n",
    "tools = [jump, get_weather]\n",
    "\n",
    "# Define the system message\n",
    "system_message = {\"role\": \"system\", \"content\": \"You are a security assistant.\"}\n",
    "\n",
    "# Initialize the Agent with the LLM client, system message, and tools\n",
    "agent = Agent(llm_client=client, system_message=system_message, tools=tools)\n",
    "\n",
    "# Run Task\n",
    "# Define a user message\n",
    "user_message = {\"role\": \"user\", \"content\": \"What is the weather in Virginia?\"}\n",
    "\n",
    "# Generate a response using the agent\n",
    "response = agent.run(user_message)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Would it Handle Multiple Tool Executions? ðŸ˜Ž\n",
    "\n",
    "You can adjust the user message to ask a question that will require a tool to run multiple times. You can enable logging to see tools being executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Executing get_weather with args: {\n",
      "  \"location\": \"Virginia\"\n",
      "}\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Executing get_weather with args: {\n",
      "  \"location\": \"Washington\"\n",
      "}\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Executing get_weather with args: {\n",
      "  \"location\": \"New York\"\n",
      "}\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatCompletionMessage(content='The weather in Virginia, Washington, and New York is currently 80F.', role='assistant', function_call=None, tool_calls=None)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Enable Logging\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Reset previous interactions\n",
    "agent.memory.reset_memory()\n",
    "\n",
    "# Define a new task\n",
    "user_message = {\"role\": \"user\", \"content\": \"What is the weather in Virginia, Washington and New York?\"}\n",
    "\n",
    "# Generate a response using the agent\n",
    "response = agent.run(user_message)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': 'What is the weather in Virginia, Washington and New York?'},\n",
       " ChatCompletionMessage(content='The weather in Virginia, Washington, and New York is currently 80F.', role='assistant', function_call=None, tool_calls=None)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.memory.get_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
