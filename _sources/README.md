# GPT Security Adventures

[![Jupyter Book Badge](https://jupyterbook.org/badge.svg)](https://cyb3rward0g.github.io/GPT-Security-Adventures/README.html)
[![Open Source Love svg1](https://badges.frapsoft.com/os/v3/open-source.svg?v=103)](https://github.com/ellerbrock/open-source-badges/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://github.com/Cyb3rWard0g/GPT-Security-Adventures/blob/main/LICENSE)

An open-source initiative to share notes, presentations, and a diverse collection of experiments presented in Jupyter Notebooks, all aimed at helping you grasp the essential concepts behind large language models and exploring the intriguing intersection of security and natural language processing.

## https://otrf.github.io/GPT-Security-Adventures/README.html

## Contributing

We encourage everyone interested in exploring the connection between security and natural language processing using large language models, such as those from the GPT family, to contribute to our project. If you have an experiment or adventure to share, please don't hesitate to submit a pull request! Consider this repository as a platform to showcase your proof of concept before creating a more advanced repository to put it into action. Your contributions are highly valued and welcomed!

## Presentations

* X33fcon 2023 - Empowering Security Teams with Generative AI: Fundamentals and Applications of GPT models ([slides](https://1drv.ms/b/s!Al3n8YlNIUPUhk0s7GVXOnA53ggE?e=N7jhVP))